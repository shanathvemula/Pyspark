{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9471d0c8-8f80-4bc6-9933-284b39fa5bf5",
   "metadata": {},
   "source": [
    "# Pyspark Tutorial 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282b464-d26e-45a0-9622-ec18943d8ec9",
   "metadata": {},
   "source": [
    "Youtube Link: https://www.youtube.com/watch?v=pOMXkbc06m4&list=PLZoTAELRMXVNjiiawhzZ0afHcPvC8jpcg&index=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "645b3818-e7a9-4f9f-a79e-4ab7a7b35816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/16 17:14:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.29.16.91:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pratices</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x70f32c1e7bc0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pratices').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e60846ae-8131-48aa-b8b6-094bba67b4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "|     NULL|NULL|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('test2.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237914f1-6519-4eca-b0a0-102e64452e13",
   "metadata": {},
   "source": [
    "## Drop Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98a580b3-a04e-4d71-bca8-08311790bd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  31|        10| 30000|\n",
      "|  30|         8| 25000|\n",
      "|  29|         4| 20000|\n",
      "|  24|         3| 20000|\n",
      "|  21|         1| 15000|\n",
      "|  23|         2| 18000|\n",
      "|NULL|      NULL| 40000|\n",
      "|  34|        10| 38000|\n",
      "|  36|      NULL|  NULL|\n",
      "|NULL|      NULL|  NULL|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387cb1e1-992f-48fd-856a-34453c8def30",
   "metadata": {},
   "source": [
    "### Dropping null values rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b530a1d-90f1-4a2b-b09b-4375053cd497",
   "metadata": {},
   "source": [
    "#### For this we can use `na.drop()`\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "* how : str, optional, the values that can be 'any' or 'all', default 'any'.\n",
    "    If 'any', drop a row if it contains any nulls.\n",
    "    If 'all', drop a row only if all its values are null.\n",
    "    \n",
    "* thresh: int, optional, default None.\n",
    "    If specified, drop rows that have less than `thresh` non-null values.\n",
    "    This overwrites the `how` parameter.\n",
    "* subset : str, tuple or list, optional\n",
    "    optional list of column names to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cb35cf4-06bd-4307-94fe-d6cbe44a021d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d93c1598-9211-409f-9eef-7d7e8409c146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99c1f020-80cb-45f5-922c-7fe7d8baf478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     NULL| 34|        10| 38000|\n",
      "|     NULL| 36|      NULL|  NULL|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='any', subset=['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c5be362-8591-4d50-84ca-67fca5ad0648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     NULL| 34|        10| 38000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='any', thresh=2, subset=['Experience', 'Salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141da46c-92f9-4c73-9c72-6e77b2bc04ae",
   "metadata": {},
   "source": [
    "Drop the rows minimum of 2 null values in Experience, Salary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe2092-5fe8-4e3f-be6b-ab3dee9a97f3",
   "metadata": {},
   "source": [
    "## Filing NULL values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7494c-7ec6-4dbc-b8b7-3158d0010485",
   "metadata": {},
   "source": [
    "### for this we can use the na.fill()\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "* value : int, float, string, bool or dict, the value to replace null values with.\n",
    "    If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
    "    from column name (string) to replacement value. The replacement value must be\n",
    "    an int, float, boolean, or string.\n",
    "* subset : str, tuple or list, optional\n",
    "    optional list of column names to consider.\n",
    "    Columns specified in subset that do not have matching data types are ignored.\n",
    "    For example, if `value` is a string, and subset contains a non-string column,\n",
    "    then the non-string column is simply ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1014c21d-acbf-44ee-a865-fc606b885c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|Miss|      Miss| 40000|\n",
      "|     Miss|  34|        10| 38000|\n",
      "|     Miss|  36|      Miss|  Miss|\n",
      "|     Miss|Miss|      Miss|  Miss|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('test2.csv', header=True)\n",
    "df.na.fill(value='Miss', subset=['Name','age','Experience','Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1964f384-81d3-4356-a66c-75ffc36e1fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "397dc60d-6fda-48d4-8b10-ac611f5beb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     Miss|  34|        10| 38000|\n",
      "|     Miss|  36|      NULL|  NULL|\n",
      "|     Miss|NULL|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('test2.csv', header=True, inferSchema=True)\n",
    "df.na.fill(value='Miss', subset=['Name','age','Experience','Salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6f5aeb-c00f-4efc-b5bc-5c1cbb098bdf",
   "metadata": {},
   "source": [
    "Here name is str and age, salary, Experience are int. <br>\n",
    "for fill we have passed the str that's way it fills only Name column missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70c833c5-1e2f-4be3-a072-a4bc60cf204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3040b91-98fa-4064-aa53-9e6a373d310b",
   "metadata": {},
   "source": [
    "#### Filling missing values with Mean, Median or mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682eabb9-b2a2-4f36-846b-711f5fec5a15",
   "metadata": {},
   "source": [
    "Filling Nan values with Mean, median or mode can be achived using the Imputer class from pyspark.ml.feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8f923a6-c635-4022-bcce-e96619ccebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/shanath_vemula/Pictures/sha/Pyspark/.venv/lib/python3.12/site-packages (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "642b2c77-58bf-4e94-a874-1621026b76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45e6065d-79dc-491e-bfef-c0850b5cbec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute = Imputer(\n",
    "    inputCols = ['age', 'Experience', 'Salary'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
    ").setStrategy('mean') # median, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b7558cd-bf8e-4558-a55a-0fc7299f7102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
      "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|         28|                 5|         40000|\n",
      "|     NULL|  34|        10| 38000|         34|                10|         38000|\n",
      "|     NULL|  36|      NULL|  NULL|         36|                 5|         25750|\n",
      "|     NULL|NULL|      NULL|  NULL|         28|                 5|         25750|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "impute.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e711d74-861a-4008-9380-065e5ca7f295",
   "metadata": {},
   "source": [
    "Or calculating the aggregate values manually and fill using `na.fill`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d36bc75d-0fb7-4e49-b080-20afcb92db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, col, mode, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bcc1cd9f-b035-46ec-b673-bdab6c6fd279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate mean for age\n",
    "value = df.select(mean(col('age'))).collect()[0][0] # return 28.5\n",
    "\n",
    "# Calculate median for age (using approxQuantile for large datasets)\n",
    "# value = df.approxQuantile('age',[0.5], 0.001)[0] # return 29.0\n",
    "\n",
    "# Calculate mode for categorical_col\n",
    "# This requires a bit more manual work to find the most frequent value\n",
    "# value = df.groupBy('Name').count().orderBy(col('count').desc()).first()[0] # returns None\n",
    "\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28cd1ec3-9d98-454d-889f-5cb8976abf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh| 28|      NULL| 40000|\n",
      "|     NULL| 34|        10| 38000|\n",
      "|     NULL| 36|      NULL|  NULL|\n",
      "|     NULL| 28|      NULL|  NULL|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(value, subset=['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "574f34cd-51e2-45fd-ae75-202dde4847f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e530bf-fcc9-4270-81ee-bee94e1f3e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a93203-c881-4e49-b314-021a2e44c287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e02f82-fdf9-4ea6-97d3-a20ab86bf4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc10e4ec-4570-4395-9c41-6c0501778060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4545332-94d6-4d67-a3df-70ca13d41d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72686a-33fd-43d4-85b7-3e208832587d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c58e9-3a57-4e3a-a654-fb1cb293d1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0af687-5d59-491e-a9a4-52669fad9a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
